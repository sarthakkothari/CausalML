{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS7290 Causal Modeling in Machine Learning: Homework 4\n",
    "Sarthak Kothari, Manthan Shah, Siddhesh Acharekar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Necessity and Sufficiency (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Probability of Neccessity and Sufficiency\n",
    "Consider the following data comparing purchases on an e-commerce website with high and low exposure to promotions.\n",
    "\n",
    "| Purchase | Promo Exposure High X=1 | Promo Exposure Low X=0 |\n",
    "| -------- | ----------------------- | ---------------------- |\n",
    "| Yes (Y=1)| 930                     | 201                    |\n",
    "| No (Y=0) | 81                      | 808                    |\n",
    "\n",
    "| Purchase | Promo Exposure High X=1 | Promo Exposure Low X=0 | Probability |\n",
    "| -------- | ----------------------- | ---------------------- | ----------- |\n",
    "| Yes (Y=1)| 0.4604                  | 0.0995                 | $P(Y=1) = 0.5599$ |\n",
    "| No (Y=0) | 0.0401                  | 808                    | $P(Y=0) = 0.4401$ |\n",
    "\n",
    "|P(Y = y, X = x)| Conditional Probabilities |\n",
    "| ------------- | ------------------------- |\n",
    "|P(Y = 1 \\| X = 0)|\t0.1992071               |\n",
    "|P(Y = 1 \\| X = 1)|\t0.9198813               |\n",
    "|P(X = 1 \\| Y = 0)|\t0.0911136               |\n",
    "|P(X = 1 \\| Y = 1)|\t0.8222812               |\n",
    "\n",
    "\n",
    "Given these data, we wish to estimate the probabilities that high exposure to promotions was a necessary (or sufficient, or both) cause of purchase. We assume monotonicity – exposure to promotions didn’t cause anyone NOT to make a purchase.\n",
    "\n",
    "We assume that purchases Y has the following simple disjunctive mechanism:\n",
    "\n",
    "\\begin{equation}\n",
    "  M =\n",
    "    \\begin{cases}\n",
    "      n_{x} \\sim \\text{BernoulliBool}(p=0.5)  \\\\\n",
    "      n_{q} \\sim \\text{BernoulliBool}(p=0.9)  \\\\\n",
    "      n_{y} \\sim \\text{BernoulliBool}(p=0.2)  \\\\\n",
    "      x = n_{x} \\\\\n",
    "      q = n_{q} \\\\\n",
    "      y = (x \\cap q) \\cup n_{y} \\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "Reformating the DAG to: \n",
    "\n",
    "We get the following state table:\n",
    "\n",
    "|Row|X|Q|$n_y$|Y|$P(Y) = P(X)P(Q)P(n_y)$|\n",
    "|-|-|-|-----|-|-|\n",
    "|R1|0|0|0|0|0.03996|\n",
    "|R2|0|0|1|1|0.00999|\n",
    "|R3|0|1|0|0|0.35964|\n",
    "|R4|0|1|1|1|0.08991|\n",
    "|R5|1|0|0|0|0.04004|\n",
    "|R6|1|0|1|1|0.01001|\n",
    "|R7|1|1|0|1|0.36036|\n",
    "|R8|1|1|1|1|0.09009|\n",
    "\n",
    "#### 1.1.1 Calculate the probability of necessity:  $P(Y_0 = 0 | X=1, Y=1)$\n",
    "\n",
    "For conditional $X = 1, Y = 1$\n",
    "\n",
    "|Row|X|Q|$n_y$|Y|$P(Y) = P(X)P(Q)P(n_y)$|\n",
    "|-|-|-|-----|-|-|\n",
    "|R6|1|0|1|1|0.01001|\n",
    "|R7|1|1|0|1|0.36036|\n",
    "|R8|1|1|1|1|0.09009|\n",
    "\n",
    "If $X = 0$, R7 changes to $Y = 0$\n",
    "so $P(Y_0 = 0 | X=1, Y=1) = \\frac{P(R7)}{P(R6) + P(R7) + P(R8)}$\n",
    "$ = \\frac{0.36036}{0.01001+0.36036+0.09009} = 0.7826$\n",
    "\n",
    "#### 1.1.2 Calculate the probability of sufficiency:  $P(Y_1 = 1 | X=0, Y=0)$\n",
    "\n",
    "For conditional $X = 0, Y = 0$\n",
    "\n",
    "|Row|X|Q|$n_y$|Y|$P(Y) = P(X)P(Q)P(n_y)$|\n",
    "|-|-|-|-----|-|-|\n",
    "|R1|0|0|0|0|0.03996|\n",
    "|R3|0|1|0|0|0.35964|\n",
    "\n",
    "If $X = 1$, R3 changes to $Y = 1$\n",
    "so $P(Y_1 = 1 | X=0, Y=0) = \\frac{P(R3)}{P(R1) + P(R3)}$\n",
    "$ = \\frac{0.35964}{0.03996+0.35964} = 0.9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Probability of Neccessity and Sufficiency, and Identifiability\n",
    "\n",
    "Typically we don’t know the whole structural model. We would only have the statistical table above, Assume only the structural assignment for Y is known.\n",
    "\n",
    "If X is exogenous and Y is monotonic relative to X, then the probabilities PN, PS, and PNS are all identifiable and are given by\n",
    "\n",
    "\\begin{align}\n",
    "PNS &= P(Y=1|X=1)−P(Y=1|X=0) \\\\\n",
    "PN &= \\frac{PNS}{P(Y=1|X=1)} \\\\\n",
    "PS &= \\frac{PNS}{P(Y=0|X=0)}\n",
    "\\end{align}\n",
    "\n",
    "#### 1.2.1 Find probability of neccessity and sufficiency in problem 1. \n",
    "$PNS = P(Y=1|X=1)−P(Y=1|X=0) = 0.9198813 - 0.1992071 = 0.7206742$\n",
    "\n",
    "#### 1.2.2  Find PN and PS using just PNS and the conditional probabilities.\n",
    "$PN = \\frac{PNS}{P(Y=1|X=1)} = \\frac{0.7206742}{0.9198813} = 0.7834426028662611$\n",
    "\n",
    "$PS = \\frac{PNS}{P(Y=0|X=0)} = \\frac{0.7206742}{1 - 0.1992071} = 0.8999507862769512$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mediation (12 points)\n",
    "\n",
    "Suppose you are a developer for a freemium subscription content platform. Your company did an A/B test for a new feature, designed to increase conversions to a paid premium subscription. The variables here are $X∈\\{0,1\\}$ for whether or not a user was exposed to the feature, and $Y∈\\{0,1\\}$ for conversion.\n",
    "\n",
    "Based on some analysis and domain knowledge, you come up with the following model.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "  C =\n",
    "    \\begin{cases}\n",
    "      X &= X_{X}  \\\\\n",
    "      T &= 3*X + N_{T}  \\\\\n",
    "      E &= 2*T + 8*X + N_{E}  \\\\\n",
    "      Y &= I(E > 10 + N_{C}) \n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "Here T is “thrash”. Since the new feature changes the website’s UX, “thrash” quantifies the time and effort the user has to spend familiarizing themselves with the new UX. E is engagement. The model assumes that the more the user engages with the site the more likely they are to convert. I is an indicator function, it returns 1 if engagement (E) > 10, 0 otherwise. Though the A/B test ties to estimate the causal effect of X on Y, T and E are mediators of that effect. You want to know how much the feature drives conversions directly through engagement, and how much is just due to thrash (which might have negative consequences on other outcomes not explicitly included in this model).\n",
    "\n",
    "$N_{X}$ comes from a fair-coin flip. All of the other noise variables are normal distributions with mean 0. However, for simplicity, we are going to assume noise variables all have a variance/standard deviation of 0. In other words, for our purposes you can assign a value of 0 to all the noise terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Calculate the total effect of the feature on conversions.\n",
    "\n",
    "$\\text{TE}(X) = E[Y|do(X=1)] - E[Y|do(X=0)]$ <br>\n",
    "When $X=0$, <br>\n",
    "$T = 0$ <br>\n",
    "$E = 0$ <br>\n",
    "$Y = I(0>10) = 0$ <br>\n",
    "\n",
    "When $X=1$ <br>\n",
    "$T = 3$ <br>\n",
    "$E = 6 + 8 = 14$ <br>\n",
    "$Y = I(14>10) = 1$ <br>\n",
    "\n",
    "So $\\text{TE}(X) = E[Y|do(X=1)] - E[Y|do(X=0)] = 1-0 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Calculate the natural indirect effect (NIE). NIE is the expected change in conversions, given no exposure to the feature, but set thrash (T) at the level it would take if one was exposed to the feature.\n",
    "\n",
    "$\\text{NIE} = E(Y_{T = 3} = 1|do(X = 0)) - E(Y_{T = 0} = 1|do(X = 0))$ <br>\n",
    "\n",
    "For $E(Y_{T = 3} = 1|do(X = 0))$ <br>\n",
    "$X = 0$\n",
    "$T = 3$\n",
    "$E = 6+0 = 6$ <br>\n",
    "$Y = I(6>10) = 0$ <br>\n",
    "\n",
    "\n",
    "For $E(Y_{T = 0} = 1|do(X = 0))$ so, <br>\n",
    "$X = 0$ <br>\n",
    "$T = 0$ <br>\n",
    "$E = 0 + 0 = 0$ <br>\n",
    "$Y = I(0>10) = 0$ <br>\n",
    "\n",
    "$\\text{NIE} = E(Y_{T = 3} = 1|do(X = 0)) - E(Y_{T = 0} = 1|do(X = 0)) = 0 - 0 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Calculate the controlled direct effect (CDE) when thrash is 0. A CDE is the effect you get when holding a mediator at a fixed value.\n",
    "\n",
    "$\\text{CDE}(T=0) = E(Y = y |do(X = 1), do(T=0)) - E(Y = y |do(X = 0), do(T=0))$ <br>\n",
    "For $X = 1$, $T = 0$ <br>\n",
    "$E = 0 + 8 = 8$ <br>\n",
    "$Y = I(8>10) = 0$ <br>\n",
    "\n",
    "For $X = 0$, $T = 0$ <br>\n",
    "$E = 0 + 0 = 0$ <br>\n",
    "$Y = I(0>10) = 0$ <br>\n",
    "\n",
    "$\\text{CDE}(T=0) = E(Y = y |do(X = 1), do(T=0)) - E(Y = y |do(X = 0), do(T=0)) = 0 - 0 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Compute the reverse natural indirect effect. Reverse NIE is expected change in conversions, given thrash (T) being fixed at feature exposure levels, but then setting X to 0. (2 points) (HINT: “change” means that going from 0 to 1 means the effect is a positive number, going from 1 to 0 means the effect has a negative number.)\n",
    "\n",
    "$\\text{Reverse NIE} = E(Y_{T = 0}|do(X = 1)) - E(Y_{T = 3}|do(X = 1))$ <br>\n",
    "\n",
    "$\\text{Reverse NIE} = 0 - 1 = -1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Compute the natural direct effect (ND) using the following formula: Total effect = NDE - reverse NIE. Explain what the implications of this is to the analysis of this feature? (1 point)\n",
    "\n",
    "Total effect = NDE - reverse NIE <br>\n",
    "NDE = Total effect + reverse NIE <br>\n",
    "So, NDE = 1 + -1 = 0 <br>\n",
    "\n",
    "**NDE is the expected increase in outcome as X changes from 0 to 1, while the mediator T is set to whatever value it would have attained prior the change, that is, under X = 0. So T shows the effect of a discriminatory variable indicating whatever the feature X is, if T is biased, the outcome Y will be biased too.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Discussion: If the noise variables were not degenerate (meaning the didn’t have non-zero variance), how would this have affected the calculations and the conclusion about the NDE? (1 point)\n",
    "\n",
    "**If noise variable were not degenerate, then they would fluctuate from expeted value. This would influence Y in one off occurences but if we were to sample a large number of times then the expected values would be the same on average.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7. Suppose instead we used the following model.\n",
    "\n",
    "\\begin{equation}\n",
    "  C =\n",
    "    \\begin{cases}\n",
    "      X &= X_{X}  \\\\\n",
    "      U &= N_{U} \\\\\n",
    "      T &= 2*X + N_{T}  \\\\\n",
    "      E &= 3*T + 7*X + N_{E}  \\\\\n",
    "      Y &= I(g(E, U, N_{C}) > \\epsilon) \n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "\n",
    "Here, U  is a vector of user-related features. g is a deep neural network that takes as input engagement (E) as well as these other user features and the noise term, and outputs a value. ϵ is a threshold. Describe in clear terms how this would change the above analysis, if at all. (2 points)\n",
    "\n",
    "**The Neural network would learn and optimize for U and E but not for T. So whatever parameters that would get learnt for U and E to maximise outcome, the mediator T would still be able to influence the outcome because it is not controlled by the network. So the discriminatory effects, if any, would be affecting the outcome no matter what E and U are learnt.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Effect of the treatment on the treated (9 points)\n",
    "\n",
    "Suppose you work for a car-sharing service company like Uber. You find that many drivers are making decisions in ways that are sub-optimal for the drivers, often missing low-hanging fruit (e.g. picking up riders closer to where they live, or choosing to drive in areas that have less demand and yet more traffic than others). If the drivers made better decisions about when and where to drive, they could make more money with a similar amount of effort.\n",
    "\n",
    "The company hires a statistical consulting company that samples some drivers for a training study. The goal of the study is to test whether a driver training program will lead drivers to make better decisions. Drivers in the study are randomly assigned to $X=1$ (recieved optimal driving training) or $X=0$ (recieved basic training that doesn’t encourage optimal descision-making). The outcome variable $Y$ is the amount of revenue the drivers earn in the study period.\n",
    "\n",
    "Let $Y_{X=1}$ be the revenue earned under exposure to the optimal training and $Y_{X=0}$ be revenue earned under exposure to baseline training. The study showed that the training is highly effected $(E(Y_{X=1}−Y_{X=0})>ϵ)$ where ϵ is some stastical significance threshold.\n",
    "\n",
    "Your team is debating whether or not you should build that training program into the mobile app. Drivers would opt-in to recieve training and guidance while driving. It would be quite expensive in terms of time and engineering resources to create this app. However, you colleagues say that the expected revenue $E(Y_{X=1}−Y_{X=0})$ would more than make up for the cost.\n",
    "\n",
    "You argue that most drivers who would opt-in are already highly motivated drivers. You think they would go on to drive more optimally by learning from their own experience, research, seeking out successful drivers, etc.\n",
    "\n",
    "To demonstrate this, you will estimate the effect of the treatment on the treated (ETT) is $E(Y_{X=1}−Y_{X=0}|X=1)$. In plain English this is the expected difference in earned revenue from those who recieved training relative to what revenue would have been had they not recieved training.\n",
    "\n",
    "The terms $Y_{X=1}$ and $Y_{X=0}$ in $E(Y_{X=1}−Y_{X=0}|X=1)$ are causal variables, in order to estimate them, you need to convert them into variables than can be estimated directly from data.\n",
    "\n",
    "The following mathematical derivations show you how to calculate ETT given $Z$, a set of valid adjustment variables that satisfy the backdoor criterion w.r.t $X$ and $Y$.\n",
    "\n",
    "Firstly, the following is true according to the basic rules of conditional probability.\n",
    "\n",
    "\\begin{align}\n",
    "P(Y_x=y|X=x′) &= \\sum_{z}P(Y_x=y,Z=z|X=x′)\\\\\n",
    "&= \\sum_{z}P(Y_x=y|Z=z,X=x′)P(Z=z|X=x′)\n",
    "\\end{align}\n",
    "\n",
    "Secondly, the counterfactual implication of the backdoor criterion is $X\\perp Y_x|Z$. This means that $P(Y_x=y|Z=z,X=x′)=P(Y_x=y|Z=z,X=x)$, because conditional on $Z$, the probability of $Y_x$ doesn’t respond to $X$. This leads to the next simplification:\n",
    "\n",
    "\\begin{align}\n",
    "P(Y_x=y|X=x′) &= \\sum_{z}P(Y_x=y,Z=z|X=x′)\\\\\n",
    "&= \\sum_{z}P(Y_x=y|Z=z,X=x′)P(Z=z|X=x′) \\\\\n",
    "&= \\sum_{z}P(Y_x=y|Z=z,X=x)P(Z=z|X=x′) \\\\ \n",
    "&= \\sum_{z}P(Y=y|Z=z,X=x)P(Z=z|X=x′)\n",
    "\\end{align}\n",
    "\n",
    "The last step is because covariate adjustment means adjusting for Z allows conditioning on $X = x$ can stand in for $\\text{do}(X = x)$.\n",
    "\n",
    "Hint: All of the terms in the last line are estimable from data. All you need to do is calculate expectations. $x’$ is the counterfactual treatment of regarding to $x$. (i.e. if $X$ is binary, $x = 0$ implies $x’ = 1$, vice versa)\n",
    "\n",
    "The data for this question is “HW4.csv”. In order to get full credit, show the work of how to calculate $E(Y_{X=0}|X=1)$ , $E(Y_{X=1}−Y_{X=0}|X=1)$, $E(Y_{X=1}−Y_{X=0})$ and then write a short paragraph to explain your results. Use $X$ as training, $Z$ as proxy for motivation, and $Y$ as revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:08.997043Z",
     "start_time": "2019-08-09T23:45:08.011456Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.046437Z",
     "start_time": "2019-08-09T23:45:08.999041Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'D:\\Northeastern_Data\\Summer_19\\CS_7290\\HW4.csv')\n",
    "data = data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.071820Z",
     "start_time": "2019-08-09T23:45:09.049161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.490090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.170279</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>7.434170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.531446</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.935765</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x         y  z\n",
       "0  0  1.490090  0\n",
       "1  1  5.170279  1\n",
       "2  1  7.434170  1\n",
       "3  0  1.531446  0\n",
       "4  0  0.935765  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.091585Z",
     "start_time": "2019-08-09T23:45:09.074678Z"
    }
   },
   "outputs": [],
   "source": [
    "# For E[Y_0|X=1]\n",
    "\n",
    "y_z0 = np.mean(data[(data['x'] == 0) & (data['z'] == 0)]['y'])\n",
    "\n",
    "y_z1 = np.mean(data[(data['x'] == 0) & (data['z'] == 1)]['y'])\n",
    "\n",
    "z1 = len(data[(data['x'] == 1) & (data['z'] == 1)])/len(data[data['x'] == 1])\n",
    "\n",
    "z0 = len(data[(data['x'] == 1) & (data['z'] == 0)])/len(data[data['x'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.119088Z",
     "start_time": "2019-08-09T23:45:09.094455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[Y0 | X = 1] =  3.0909474283549194\n"
     ]
    }
   ],
   "source": [
    "print('E[Y0 | X = 1] = ', (y_z1 * z1) + (y_z0 * z0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.140989Z",
     "start_time": "2019-08-09T23:45:09.121664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[Y | X == 1] =  4.436555490118973\n"
     ]
    }
   ],
   "source": [
    "print('E[Y | X == 1] = ', str(np.mean(data[data['x'] == 1]['y'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ETT = E(Y_{X=1}−Y_{X=0}|X=1) = E[Y | X = 1] - \\sum_{z} E[Y|X=0,Z=z]P(Z=z|X=1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.156974Z",
     "start_time": "2019-08-09T23:45:09.140989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETT =  1.3456080617640538\n"
     ]
    }
   ],
   "source": [
    "ETT = ((np.mean(data[data['x'] == 1]['y']))) - ((y_z1 * z1) + (y_z0 * z0))\n",
    "print('ETT = ', ETT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can clearly see that the expected difference in earned revenue from those who recieved training relative to what revenue would have been had they not recieved training is positive. Therefore the training program is beneficial.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.177937Z",
     "start_time": "2019-08-09T23:45:09.159276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[Y1 -Y0] =  0.6701128147584988\n"
     ]
    }
   ],
   "source": [
    "print('E[Y1 -Y0] = ', ETT * (len(data[data['x'] == 1]) / len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E(Y_{X=1}−Y_{X=0}) = E[Y_{X=1}] - E[Y_{X=0}]$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E[Y_{X=1}] = E[Y_{X=1}|X=0]P(X=0) + E[Y_{X=1}|X=1]P(X=1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.199577Z",
     "start_time": "2019-08-09T23:45:09.180485Z"
    }
   },
   "outputs": [],
   "source": [
    "# For E[Y_1|X=0]\n",
    "\n",
    "y_z0 = np.mean(data[(data['x'] == 1) & (data['z'] == 0)]['y'])\n",
    "\n",
    "y_z1 = np.mean(data[(data['x'] == 1) & (data['z'] == 1)]['y'])\n",
    "\n",
    "z0 = len(data[(data['x'] == 0) & (data['z'] == 0)])/len(data[data['x'] == 0])\n",
    "\n",
    "z1 = len(data[(data['x'] == 0) & (data['z'] == 1)])/len(data[data['x'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.221817Z",
     "start_time": "2019-08-09T23:45:09.201885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0754568551193047\n"
     ]
    }
   ],
   "source": [
    "e_y_x_1 = (y_z0*z0 + y_z1*z1)*(len(data[data['x'] == 0])/len(data)) + np.mean(\n",
    "    data[data['x'] == 1]['y'])*(len(data[data['x'] == 1])/len(data))\n",
    "print(e_y_x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E[Y_{X=0}] = E[Y_{X=0}|X=1]P(X=1) + E[Y_{X=0}|X=0]P(X=0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.240396Z",
     "start_time": "2019-08-09T23:45:09.224099Z"
    }
   },
   "outputs": [],
   "source": [
    "# For E[Y_0|X=1]\n",
    "\n",
    "y_z0 = np.mean(data[(data['x'] == 0) & (data['z'] == 0)]['y'])\n",
    "\n",
    "y_z1 = np.mean(data[(data['x'] == 0) & (data['z'] == 1)]['y'])\n",
    "\n",
    "z1 = len(data[(data['x'] == 1) & (data['z'] == 1)])/len(data[data['x'] == 1])\n",
    "\n",
    "z0 = len(data[(data['x'] == 1) & (data['z'] == 0)])/len(data[data['x'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.264686Z",
     "start_time": "2019-08-09T23:45:09.243069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.861055737142217\n"
     ]
    }
   ],
   "source": [
    "e_y_x_0 = (y_z1 * z1 + y_z0 * z0)*(len(data[data['x'] == 1])/len(data)) + np.mean(\n",
    "    data[data['x'] == 0]['y'])*(len(data[data['x'] == 0])/len(data))\n",
    "print(e_y_x_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $E(Y_{X=1}−Y_{X=0}) = E[Y_{X=1}] - E[Y_{X=0}]$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-09T23:45:09.277384Z",
     "start_time": "2019-08-09T23:45:09.269393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E(Y1 - Y0) =  1.2144011179770877\n"
     ]
    }
   ],
   "source": [
    "print(\"E(Y1 - Y0) = \", e_y_x_1 - e_y_x_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T21:52:23.428896Z",
     "start_time": "2019-08-08T21:52:23.421904Z"
    }
   },
   "source": [
    "**If the cost for the app is less than 1.214, then the app should be built since the expected revenue would more than make up for the cost. This conclusion is drawn from observed data.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
